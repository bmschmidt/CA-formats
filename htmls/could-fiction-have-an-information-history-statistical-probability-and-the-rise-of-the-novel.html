
    <head>
    <meta name="author" content="Dallas Liddle">
    <title>Could Fiction Have an Information History?  Statistical Probability and the Rise of the Novel</title>
    <meta name="date" content="02.21.19">
    <meta name="shortauthor" content="Dallas Liddle">
    <meta name="shorttitle" content="Could Fiction Have an Information History?  Statistical Probability and the Rise of the Novel">
    </head>
    
	<div class="entry print-only">
						 
		 
		<div class="post-2155 post type-post status-publish format-standard has-post-thumbnail hentry category-articles" id="post-2155">
			<!-- Show post on single page if option enabled -->

		
					<h6><em>Peer-Reviewed By: Katherine Bode, David Bamman</em></h6>
<h6><em>Clusters: <a href="http://culturalanalytics.org/2016/05/genre/" target="_blank" rel="noopener noreferrer">Genre</a></em></h6>
<h6><em>Article DOI: <a href="http://doi.org/10.22148/16.033" target="_blank" rel="noopener noreferrer">10.22148/16.033</a></em></h6>
<h6><em>Dataverse DOI: <a href="https://doi.org/10.7910/DVN/M7AGWJ" target="_blank" rel="noopener noreferrer">10.7910/DVN/M7AGWJ</a></em></h6>
<h6><em>PDF DOI: <a href="https://doi.org/10.31235/osf.io/6wxyv" target="_blank" rel="noopener noreferrer">10.31235/osf.io/6wxyv</a></em></h6>
<h6><em>Journal ISSN: 2371-4549</em></h6>
<h6><em>Cite: Dallas Liddle, "Could Fiction Have an Information History? Statistical Probability and the Rise of the Novel," Journal of Cultural Analytics. February 21, 2019. DOI: 10.31235/osf.io/6wxyv</em></h6>
<p> </p>
<p style="text-align: right;">                                              "More matter with less art."- <em>Hamlet </em>2.2</p>
<p>Literary scholars in the information age, whatever issues we may differ on, have long shared a belief that the amount of "information" in literary language is not a phenomenon that can be empirically investigated.<sup><a href="#footnote_0_2155" id="identifier_0_2155" class="footnote-link footnote-identifier-link" title="The author is grateful to the editor and reviewers of CA for generous feedback on earlier drafts of this article, which led me to ask new questions, run new tests, and seek out resources I had entirely missed. Everything still missing in the current version is my sole responsibility. Some parts of the work described here were supported by a 2010 NEH Summer Stipend, others by a 2016 Advanced Collaborative Support Grant from the HathiTrust Research Center.">1</a></sup> We tell our students, and remind each other, that the meaning in literary texts is inherently subjective, culture-specific, and contextual rather than fixed. Scholar Katherine Bode reaffirms this article of faith in a new book on digital literary methodology, writing that "literary data are inevitably constructed and transactional, whether they are explicitly designated so or not."<sup><a href="#footnote_1_2155" id="identifier_1_2155" class="footnote-link footnote-identifier-link" title="Katherine Bode, A World of Fiction: Digital Collections and the Future of Literary History (Ann Arbor: University of Michigan Press, 2018), 96.">2</a></sup> Long prose forms such as the novel seem especially immune to empirical assessment. As Thomas Pavel notes, the wide-open form of the novel allows "for any imaginable kind of confabulation without constraint,"<sup><a href="#footnote_2_2155" id="identifier_2_2155" class="footnote-link footnote-identifier-link" title="Thomas G. Pavel, Fictional Worlds (Cambridge: Harvard University Press, 1986), 2.">3</a></sup> while the fact that each reader can reinterpret and recontextualize a novel anew seems to guarantee that the meanings latent in such texts can never be fully described, much less quantified. Probably because both digital and traditional humanists hold these beliefs, most data mining, topic modeling, sentiment analysis, and other distant reading methods used on fiction search for subtle patterns or relative thematic trends across time, or for faint signals of style detectable in genres or the works of individual writers. We assume that the total quantity of information contained in novels could never be computationally estimated by any measure humanists would care about.</p>
<p>The reported outcome of a digital humanities experiment by Mark Algee-Hewitt of the Stanford Literary Lab, published online in the multi-authored Pamphlet 11, "Canon/Archive: Large-Scale Dynamics in the Literary Field" (2016), is therefore both anomalous and interesting.<sup><a href="#footnote_3_2155" id="identifier_3_2155" class="footnote-link footnote-identifier-link" title="Mark Algee-Hewitt, Sarah Allison, Marissa Gemma, Ryan Heuser, Franco Moretti, Hannah Walser, &quot;Canon/Archive: Large-scale Dynamics in the Literary Field,&quot; Stanford Literary Lab Pamphlet 11. Jan. 2016.">4</a></sup> Algee-Hewitt and colleagues compared two large sets of nineteenth-century novels: 250 works in a proprietary e-text collection,<sup><a href="#footnote_4_2155" id="identifier_4_2155" class="footnote-link footnote-identifier-link" title="For a description of the collection see &quot;About Nineteenth-Century Fiction,&quot; Chadwyck-Healey Literature Collections. c. 1996-2016.">5</a></sup>representing the canon, and a larger number of texts from the general population of novels published in the same era, representing the archive. They divided all texts into collections of bigrams (word pairs) which they assessed for mathematical "redundancy" using equations based on the discipline of Information Theory founded by Bell Systems engineer Claude Shannon in 1948.<sup><a href="#footnote_5_2155" id="identifier_5_2155" class="footnote-link footnote-identifier-link" title="The mathematical Information Theory invoked and used by Algee-Hewitt et al. is not a standard area of undergraduate mathematics, but a number of excellent introductions and guides have been written. For a brief and readable overview I recommend the background section of M. B. Plenio and V. Vitelli, &quot;The physics of forgetting: Landauer’s erasure principle and information theory,&quot; Contemporary Physics 42.1 (2001): 25-60, available as a PDF from the Imperial College London website. A longer introduction aimed at the generalist is John R. Pierce, An Introduction to Information Theory: Symbols, Signals, and Noise, rev. ed. (New York: Dover, 1980), which was reviewed in manuscript by Claude Shannon. The standard textbook for Information Theory at the graduate level is Thomas M. Cover and Joy A. Thomas, Elements of Information Theory (New York and Chichester: John Wiley, 1991). To go back to the source see Claude E. Shannon, &quot;A Mathematical Theory of Communication,&quot; The Bell System Technical Journal 27 (July and October 1948): 379-423, 623-656, a reprint of which is available on the website of the Mathematics department at Harvard.">6</a></sup>The canonical novels displayed different statistical characteristics than the archival ones, and the effect was large: "that three-fourths of the Chadwyck-Healey collection would be less redundant than three-fourths of the archive was a <em>much </em>stronger separation than we had expected to find."<sup><a href="#footnote_6_2155" id="identifier_6_2155" class="footnote-link footnote-identifier-link" title="Algee-Hewitt et al., 6.">7</a></sup>The Stanford authors seem to have found that for nineteenth-century fiction a purely statistical feature of texts<span class="emdash">—</span>the redundancy of their words, indicating higher or lower information density in mathematical terms<span class="emdash">—</span>has a strange power to predict the value that would be assigned to those texts by historical readers, markets, and scholars.</p>
<p>The Stanford authors' somewhat conservative interpretation of this result was to see it as confirming literary conventional wisdom: of course less-celebrated authors would use language more redundantly. "The clarity of the contrast had simply confirmed a received idea: forgotten authors used language in a redundant fashion; if they had remained unread, it was because they weren't really <em>worth </em>reading [....] Not exciting, corroborating a received idea."<sup><a href="#footnote_7_2155" id="identifier_7_2155" class="footnote-link footnote-identifier-link" title="Algee-Hewitt et al., 6.">8</a></sup> When they attempted to determine more exactly what canonical novels did differently than archival ones, however, the Stanford authors found the unreadably large lists of bigrams generated by their method a methodological impasse: "Here, statistical significance seemed impervious to critical meaningfulness."<sup><a href="#footnote_8_2155" id="identifier_8_2155" class="footnote-link footnote-identifier-link" title="Algee-Hewitt et al., 7.">9</a></sup> They proceeded to lexical (type-token) and grammatical (part of speech) comparisons of the canon and archive texts, but these generated less striking results, and they concluded that their bigram redundancy test had led to a dead end.</p>
<p>In presenting their experiment as interesting but uninterpretable Algee-Hewitt et al. demonstrate modesty and scholarly scrupulousness, but seem to miss the most significant implications of their result. It is true that we expect skilled writers not to write redundantly, but not true that any reputable literary humanist of the past half-century familiar with what Claude Shannon called the mathematically-defined "information" of a message would expect the <em>semantic </em>redundancy human readers recognize in literary texts to covary meaningfully with the <em>statistical </em>redundancy computers can calculate, or that any mathematical analysis could predict a judgment as complex, contingent, and human as the literary value of a book. Generations of scholars have believed the opposite. A young Noam Chomsky, formulating his own framework for linguistic investigation in the 1950s, coined the nonsense line "Colorless green ideas sleep furiously" specifically to demonstrate that mathematical information theory's probabilistic approach could never explain high-order human use of language.<sup><a href="#footnote_9_2155" id="identifier_9_2155" class="footnote-link footnote-identifier-link" title="Noam Chomsky, &quot;Three Models for the Description of Language,&quot; IRE Transactions on Information Theory 2.3 (1956), 116. ">10</a></sup> After Chomsky, Fr. Walter Ong attacked information theory as simplistic and mechanistic in a succession of books and articles over more than a decade, while Colin Cherry, who had experimented with it, came to describe the theory as a "blind alley" for human communication.<sup><a href="#footnote_10_2155" id="identifier_10_2155" class="footnote-link footnote-identifier-link" title="Cherry told an interviewer in the 1970s that Information Theory &quot;doesn't help us&quot; much to understand human communication [....] It was a blind alley.&quot; See Carol Wilder, &quot;A Conversation with Colin Cherry,&quot; Human Communication Research 3.4 (1977), 356.">11</a></sup>The belief that information theory was useless for humanist inquiry seemed to have the endorsement even of Claude Shannon, whose 1948 paper on the mathematics of communication stipulated that "semantic aspects of communication are irrelevant to the engineering problem" with which his theory was concerned.<sup><a href="#footnote_11_2155" id="identifier_11_2155" class="footnote-link footnote-identifier-link" title="Shannon's position on this issue is usually overstated, however. While he did write that the semantic aspects of communication were irrelevant to the engineering aspects, he never claimed the engineering aspects were equally irrelevant to semantic communication. The metaphor that statistical information is &quot;divorced&quot; from meaning is therefore misapplied. Divorces make a mutual separation; Shannon's claim of separation went only one direction.">12</a></sup> The current 3rd edition of the <em>Oxford English Dictionary </em>defines "information" in the statistical sense Shannon established (2c.) as "a mathematically defined quantity divorced from any concept of news or meaning," and adduces a quotation from M.A.K. Halliday that "Information theory, which has a place in the quantitative description of a language, implies nothing about the relative efficiency of languages or the effectiveness of language activity."<sup><a href="#footnote_12_2155" id="identifier_12_2155" class="footnote-link footnote-identifier-link" title="&quot;information, n.&quot; OED Online. June 2017. Oxford University Press. (accessed December 21, 2017).">13</a></sup>The leading authority on cybernetics in modern fiction, N. Katherine Hayles, has long opposed use of information mathematics for literary study, writing as recently as 2014 that for humanists "a theory totally divorced from meaning has little to contribute."<sup><a href="#footnote_13_2155" id="identifier_13_2155" class="footnote-link footnote-identifier-link" title="N. Katherine Hayles, &quot;Cognition Everywhere: The Rise of the Cognitive Nonconscious and the Costs of Consciousness,&quot; New Literary History 45 (2014), 216-217.">14</a></sup></p>
<p>The consensus of so many major literature and communications scholars would carry weight even if the idea of a relationship between statistical information and literary value did not seem so unlikely on its face. It is counter-intuitive to say the least to imagine that a Victorian publisher circa 1850 who divided the texts of novel manuscripts into bigrams and laboriously calculated and summed the likelihood <em>p<sub>i </sub></em>that the bigrams would appear, using Shannon's equation for the information content of a message source,<a href="http://culturalanalytics.org/2019/02/could-fiction-have-an-information-history-statistical-probability-and-the-rise-of-the-novel/liddle_formula/" rel="attachment wp-att-2158"><img class="aligncenter wp-image-2158" src="http://culturalanalytics.org/wp-content/uploads/2019/01/liddle_formula.png" alt="" width="175" height="59" srcset="https://culturalanalytics.org/wp-content/uploads/2019/01/liddle_formula.png 417w, https://culturalanalytics.org/wp-content/uploads/2019/01/liddle_formula-300x101.png 300w" sizes="(max-width: 175px) 100vw, 175px" /></a>would have had in <em>H </em>a reasonably good indicator of which works might go on to become classics. The Algee-Hewitt et al. result suggests that, despite all intuition, this would have been true. Their finding of an apparent association between subjective literary value and objective information density has potential to call some of literary studies' longest-standing assumptions into question.</p>
<p>It should be emphasized that the experiments that follow exploring this potential association do not attempt to directly replicate the Stanford methods<span class="emdash">—</span>fortunately so, since Pamphlet 11 is thin on the methodological details that would enable replication.<sup><a href="#footnote_14_2155" id="identifier_14_2155" class="footnote-link footnote-identifier-link" title="Neither Pamphlet 11 nor the Literary Lab website share the original results, list of archival texts, or the code of the algorithm used to generate their redundancy figures. Like the other Stanford pamphlets, the work was also not subjected to peer review. For the reasoning behind the latter decision see Franco Moretti, &quot;Literature, Measured,&quot; Stanford Literary Lab Pamphlet 12, April 2016.">15</a></sup> As a historian of genre and the nineteenth-century novel my interest is less with quantifying canonicity than with studying how or whether the information dynamics Algee-Hewitt et al. may have detected could have influenced historical authorship and the novel's development. Technology historians established decades ago that competitive pressure to employ more mathematically efficient codes shaped the development of communications technologies from signal beacons and semaphore flags to the telegraph, telephone, and fiber-optic cable.<sup><a href="#footnote_15_2155" id="identifier_15_2155" class="footnote-link footnote-identifier-link" title="An excellent study of how the development of digital and non-digital communication networks were influenced by the same patterns and forces is Gerard J. Holzmann and Björn Pehrson, The Early History of Data Networks (Washington and Brussels: IEEE Computer Society Press, 1995).">16</a></sup> In recent years literary scholars have sometimes characterized the novel as a technology also.<sup><a href="#footnote_16_2155" id="identifier_16_2155" class="footnote-link footnote-identifier-link" title="See for example Richard Menke, Telegraphic Realism: Victorian Fiction and Other Information Systems (Stanford: Stanford UP, 2008); Cara Murray, Victorian Narrative Technologies in the Middle East (New York: Routledge, 2008); and Tony Jackson, The Technology of the Novel: Writing and Narrative in British Fiction (Baltimore: Johns Hopkins UP, 2009).">17</a></sup> But no researcher seems to have carried the metaphor far enough to ask what it now seems possible to at least ask: whether the mathematics of information could have shaped the developing practices of novelists as well as those of engineers and coders.</p>
<p> </p>
<h1 style="text-align: center;">Measuring the "information" in nineteenth-century novels: method and results</h1>
<p>Luckily for literary historians looking for ways to test a potential relationship between Information Theory and literary history, theoretical and applied computational linguists have had fewer objections to statistical information than their humanist counterparts. The reader probably uses many practical achievements of computational linguistics daily: spelling correction in word processors, next-word predictors in text messaging, spam filters in email, and speech recognition tools in digital assistants are all built on information-theoretical algorithms. As Kenneth W. Church recently explained, Shannon's "noisy-channel model makes lots of sense for speech recognition, OCR, and spelling correction" while Naïve Bayesian statistical algorithms are valuable for spam filtering, author identification, sentiment analysis, and word sense disambiguation.<sup><a href="#footnote_17_2155" id="identifier_17_2155" class="footnote-link footnote-identifier-link" title="Kenneth W. Church, “Statistical Models for Natural Language Processing,” Oxford Handbook of Computational Linguistics 2nd edition, ed. Ruslan Mitkov. Online publication date: Aug. 2016. DOI: 10.1093/oxfordhb/9780199573691.013.54.">18</a></sup>The assumption behind most of these systems is that they are only ways to enable computers to <em>seem </em>to process language as humans do, however, not that they reproduce or even illuminate real human processes. Church notes that speech recognition systems distinguish ambiguous words using "confusion matrices" created by supervised algorithms that have processed trillions of words of natural-language corpora<span class="emdash">—</span>not quite the way people do it.</p>
<p>Researchers in comparative linguistics have established at least one method by which an information-theoretical tool may help illuminate issues of human language use, however. Patrick Juola, Max Bane, and others have used a simple information-theoretical algorithm to measure the information densities of the same document across different languages, such as translations of the Bible and European Union Constitution, as they explore the question of whether all human languages are equally complex.<sup><a href="#footnote_18_2155" id="identifier_18_2155" class="footnote-link footnote-identifier-link" title="See for example Max Bane, “Quantifying and Measuring Morphological Complexity,” in Proceedings of the 26th West Coast Conference on Formal Linguistics, ed. Charles B. Chang and Hannah J. Haynie (Somerville, MA: Cascadilla Proceedings Project, 2008), 69-76. Available: www.lingref.com, document #1657. Patrick Juola, “Assessing linguistic complexity,” is in Matti Miestamo, Kaius Sinnemaki and Fred Karlsson, eds., Language Complexity: Typology, Contact, Change (Amsterdam/Philadelphia: Benjamins, 2008), 89-108. The compressibility of 21 different translations of the EU Constitution was tested by Markus Sadeniemi, Kimmo Kettunen, Tiina Lindh-Knuutila, and Timo Honkela in “Complexity of European Union Languages: A Comparative Approach,” Journal of Quantitative Linguistics 15.2 (2008): 185-211.">19</a></sup> The quantity these researchers seek to determine for each translation of the same text is its <em>Kolmogorov complexity</em>, an alternative way to determine Shannon information in which the information content of a text string is defined as the length of the shortest computer program that would output that string and then stop. To show what that means, these two text strings both have exactly 38 ASCII characters:</p>
<p style="padding-left: 30px;">[1] CoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCo</p>
<p style="padding-left: 30px;">[2] Colorless green ideas sleep furiously.</p>
<p>The strings may contain the same absolute number of text characters<em>, </em>but do not have the same quantity of mathematical information. In Shannon's original terms, that is because the first string is so predictable<span class="emdash">—</span>after reading the first few characters the reader could easily pick the next in line until the end. In Kolmogorov's terms, the same attribute can be measured by finding the length of the shortest computer command needed to recreate each string. String [1] is so highly patterned and predictable that it could be reproduced by a simple instruction to print "Co" 19 times. String [2] is so unpatterned and unpredictable that it would have to be reproduced at close to full length as part of any machine instruction. String [2] contains greater mathematical information than string [1] by both Shannon's and Kolmogorov's criteria of measurement.</p>
<p>An exact final value for the Kolmogorov complexity of a text string is formally incomputable,<sup><a href="#footnote_19_2155" id="identifier_19_2155" class="footnote-link footnote-identifier-link" title="See Ming Li and Paul Vitányi, An Introduction to Kolmogorov Complexity and its Applications, 2nd ed. (New York and Berlin, Springer, 1997).">20</a></sup> but linguists consider it well established that Kolmogorov complexity and compressibility are so nearly the same phenomenon<sup><a href="#footnote_20_2155" id="identifier_20_2155" class="footnote-link footnote-identifier-link" title="“Fortunately, it can be shown that all reasonable choices of programming languages lead to quantification of the amount of ‘absolute’ information in individual objects that is invariant up to an additive constant. We call this quantity the ‘Kolmogorov complexity’ of the object. If an object contains regularities, then it has a shorter description than itself. We call such an object ‘compressible.’” Li and Vitányi, v.">21</a></sup> that the Kolmogorov complexity of a text can be estimated using off-the-shelf file compression programs such as WinRAR, WinZip, gzip, and Zlib.<sup><a href="#footnote_21_2155" id="identifier_21_2155" class="footnote-link footnote-identifier-link" title="Ehret and Szmrecsanyi use the utility gzip, while Sadeniemi et al. use Bzip2. Juola, who has used both, reports they produce similar results; see Juola, 102n3.">22</a></sup>As Juola writes, "If [...] Kolmogorov complexity represents the ultimate possible file compression, a good file compressor can be seen as an attempt to approximate this kind of complexity within a tractable formal framework."<sup><a href="#footnote_22_2155" id="identifier_22_2155" class="footnote-link footnote-identifier-link" title="Juola, “Assessing Linguistic Complexity,” 92.">23</a></sup> By compressing multiple translations of <em>Alice in Wonderland </em>as tightly as possible and comparing the ratios of the full-sized and compressed versions, Katharina Ehret and Benedikt Szmrecsanyi produced a table of comparisons of ten languages' relative complexity in line with what more orthodox complexity notions would lead one to expect" (e.g. Hungarian led the overall list).<sup><a href="#footnote_23_2155" id="identifier_23_2155" class="footnote-link footnote-identifier-link" title="Katharina Ehret and Benedikt Szmrecsany, “An Information-theoretic approach to assess linguistic complexity,” in R. Baechler and G. Seiler, eds, Complexity, isolation, and variation (Berlin/Boston, MA: De Gruyter, 2016), 74.">24</a></sup> The method is not limited to use on parallel texts, however. In a different project they themselves call a "proof-of-concept study," Ehret and Szmrecsany assessed essays by second-language learners of English at various degrees of mastery, finding that "essays by more advanced learners tend to be more Kolmogorov complex than essays by less advanced learners."<sup><a href="#footnote_24_2155" id="identifier_24_2155" class="footnote-link footnote-identifier-link" title="Katharina Ehret and Benedikt Szmrecsanyi, “Compressing learner language: An information-theoretic measure of complexity in SLA production data,” in Second Language Research (2016): 10.">25</a></sup></p>
<p>In the experiments below, instead of using a word-level assessment such as the bigram tool the Stanford researchers employed, full-text corpora of large numbers of novels were assessed for the compressibility of their texts at the more fundamental level of the byte and ASCII code character by the Kolmogorov complexity method used by the linguists. The compression utility chosen was WinRAR, set to best possible compression using the "Zip" method, so that the algorithm applied would be DEFLATE. This industry-standard compressor uses a combination of Huffman coding, which finds the relative rates at which alphanumeric characters, read as codes in ASCII, are used overall in a file, and substitutes the most space-efficient codes possible (the commonest characters get the shortest codes, the rarest characters the longest), and Lempel-Ziv coding, which searches for repeated strings of code at the byte level and replaces these with index markers, creating a much smaller file capable of re-generating the original text. Text files packed by such combinations of Huffman and Lempel-Ziv coding often require as little as a third of the storage space needed by the original file.</p>
<p>To make the experimental praxis as transparent as possible, only freely available textual archives have been used, so that any reader with an Internet connection should be able to replicate the experiments. Large sets of texts were also sought; Ehret and Szmrecsanyi note that the compressibility ratios of individual files are hard to interpret in the abstract, and best evaluated in comparative contexts.<sup><a href="#footnote_25_2155" id="identifier_25_2155" class="footnote-link footnote-identifier-link" title="Ehret and Szmrecsanyi, “Compressing Learner Language,” 19.">26</a></sup>All other tools and methods used are identified as they appear, and all spreadsheets used to create the visualizations shared. For terminology, since the numbers calculated are actually file compression ratios, that is the language I will use for them, rather than claiming that they represent (rather than approximate) Kolmogorov complexity.</p>
<p>A first attempt to assess the evidence for information-theoretical pressures on the historical development of the novel using this method was performed on Andrew Piper's freely available .txtLAB collection of 451 novels in English, French, and German published between the 1770s and 1930s.<sup><a href="#footnote_26_2155" id="identifier_26_2155" class="footnote-link footnote-identifier-link" title="Andrew Piper, txtlab Multilingual Novels, 2016. Figshare.">27</a></sup> Each novel in the sets of 150 or 151 novels from each national tradition was compressed with DEFLATE with WinRAR and the ratio of full-size to compressed size for each work in the English, French, and German collections calculated and displayed over time. The strong expectation literary historians should surely have for such an experiment<span class="emdash">—</span>and also the null hypothesis<span class="emdash">—</span>would be that the machine compressibility of a given literary text will turn out to be meaningless metadata, which is how literary scholars have always treated it. Compressibility from text to text could be expected to vary slightly based on random factors and stylistic peculiarities of individual works and authors, but seems unlikely to trend over time, and the corpus as a whole should show noise rather than signal. A result that found a real trend in such data, however, would suggest that the average information density of fiction in one or more of these national traditions really did change over time, though it would not in itself identify the reason.</p>
<p>Scatterplots of the relative compressibility over time for the English, French, and German novels in the .txtLAB collection are visualized side by side in Figure 1 below, made using R with ggplot2.<sup><a href="#footnote_27_2155" id="identifier_27_2155" class="footnote-link footnote-identifier-link" title="R: A Language and Environment for Statistical Computing, R Core Team, R Foundation for Statistical Computing, Vienna, Austria, 2018. https://www.R-project.org; and H. Wickham, “ggplot2: Elegant Graphics for Data Analysis” (New York: Springer-Verlag, 2016).">28</a></sup> The reader is encouraged to download the original archive from Figshare, and/or the spreadsheet associated with Figure 1, to reproduce and explore the patterns for themselves.</p>
<div id="attachment_2159" style="width: 650px" class="wp-caption aligncenter"><a href="http://culturalanalytics.org/2019/02/could-fiction-have-an-information-history-statistical-probability-and-the-rise-of-the-novel/liddle_figure1/" rel="attachment wp-att-2159"><img aria-describedby="caption-attachment-2159" class="wp-image-2159" src="http://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure1.jpg" alt="" width="640" height="192" srcset="https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure1.jpg 3000w, https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure1-300x90.jpg 300w, https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure1-768x230.jpg 768w, https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure1-1024x307.jpg 1024w" sizes="(max-width: 640px) 100vw, 640px" /></a><p id="caption-attachment-2159" class="wp-caption-text">Figure 1. Compression ratios ("packed" size divided by "unpacked" size) of 150 German, 150 English, and 151 French novels published between the 1770s and 1930s. Higher numbers indicate lower compressibility. Source of data: Andrew Piper, .txtLAB.</p></div>
<p>All three visualizations are noisy, but all also suggest an upward trend over time toward less compressibility and greater density of information in each nation's novels. To assess whether the trends are real or only statistical noise, Mann-Kendall tests were performed using A.I. McLeod's R package Kendall.<sup><a href="#footnote_28_2155" id="identifier_28_2155" class="footnote-link footnote-identifier-link" title="A.I. McLeod, &quot;Package 'Kendall.'&quot; 16 May 2011.">29</a></sup> Mann-Kendall is a test for change in independently collected observations, often used to look for trends in time series of environmental samples. The result for the German novels was to show only a slight upward trend (tau = 0.0633) in information density, and not to a statistically significant degree (2-sided pvalue = 0.25128). The English novels, however, showed a stronger upward trend (tau = 0.183) at high statistical significance (2-sided pvalue = 0.0009166). The French novels showed an even more marked upward trend (tau = 0.289) and higher statistical significance (2-sided pvalue = 0.00000011921). The apparent increase over time in the information density of English and French fiction, at least, does not initially seem to be a function of random variation alone.</p>
<p>Statistically significant or not, the English and French results are so odd that it seemed important to try to rule out alternative explanations for what seems to be a shared movement by literally hundreds of French and British novelists to write more mathematically information-dense fiction as the nineteenth century went on. Other explanations for these numbers than a genre-wide evolution toward more dense information coding practices for fiction are possible, and more intuitively likely. Perhaps over this period the average novel in both countries got longer or shorter in absolute terms, and the apparent increase in information density only shows the compression utility's efficiency in packing larger or smaller files. Perhaps discourse in this period underwent a more general cultural change, and novelists' changing language reproduced a larger cultural trend toward more varied, concrete, or otherwise information-dense expression. When Stanford's Ryan Heuser and Long Le-Khac discovered a trend away from abstract words and toward more concrete ones in British fiction over this same period, a cultural trend was their preferred explanation, and other scholars have found this idea convincing.<sup><a href="#footnote_29_2155" id="identifier_29_2155" class="footnote-link footnote-identifier-link" title="See Ryan Heuser and Long Le-Khac, &quot;A Quantitative Literary History of 2,958 Nineteenth-Century British Novels: The Semantic Cohort Method,&quot; Literary Lab Pamphlet 4, May 2012. Alan Liu's endorsement of their approach appears in &quot;The Meaning of the Digital Humanities,&quot; PMLA 128 (2013): 409-423. More recently Matt Erlin, in &quot;Topic Modeling, Epistemology, and the English and German Novel,&quot; Journal of Cultural Analytics 1 May 2017, Article DOI: 10.22148/16.014, Dataverse DOI: 10.7910/DVN/3J38FX, interprets new research findings of his own as corroborating theirs.">30</a></sup></p>
<p>To test the first alternative explanation for the increasing information density of these novels, that the effect is due only to novels changing in absolute size, the simple uncompressed file size of all three sets of novels was visualized and more Mann-Kendall trend tests run.</p>
<div id="attachment_2165" style="width: 650px" class="wp-caption aligncenter"><a href="http://culturalanalytics.org/2019/02/could-fiction-have-an-information-history-statistical-probability-and-the-rise-of-the-novel/liddle_figure2/" rel="attachment wp-att-2165"><img aria-describedby="caption-attachment-2165" class="wp-image-2165" src="http://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure2.jpg" alt="" width="640" height="192" srcset="https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure2.jpg 3000w, https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure2-300x90.jpg 300w, https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure2-768x230.jpg 768w, https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure2-1024x307.jpg 1024w" sizes="(max-width: 640px) 100vw, 640px" /></a><p id="caption-attachment-2165" class="wp-caption-text">Figure 2. Absolute size of the original text files in Andrew Piper's collection of 451 English, French, and German novels.</p></div>
<p>In all three cases the average size of each nation's novels decreased very slightly over time (English tau -0.0939, French tau -0.008539, German tau -0.0962), but none of the decreases was statistically significant at p &lt; 0.05 (2-sided pvalues 0.088572, 0.87979, 0.080945). When the relationship between compressibility and original file size was checked with the full suite of correlation tests available in R (Pearson, Kendall, and Spearman), however, the compression ratio of the French novels, though not the others, did covary significantly with the original size of the file. This should not have been unexpected, since many more French novels in the Piper collection are short, and the comparative linguists who have used this method observe that file compression can behave erratically on smaller files.<sup><a href="#footnote_30_2155" id="identifier_30_2155" class="footnote-link footnote-identifier-link" title="Ehret and Szmrecsanyi, “Compressing Learner Language,” 19.">31</a></sup>To control for this effect the original texts of all three sets of national novels were concatenated into single very large text files which were then divided into exactly equal 1-megabyte slices (99 English, 77 French, 94 German). The compression algorithm was re-run on the slices, with the result shown in Figure 3. When more Mann-Kendall tests were performed, English fiction continued to show a statistically significant rise over the period (tau = .199, 2-sided pvalue = 0.0034964), French fiction maintained its still more significant increase (tau = .403, 2-sided pvalue = 2.3842e-07), and German fiction continued to show no significant effect over time (tau = -0.03, 2-sided pvalue = 0.67112). The apparent historical increase in the information density of French and English novels does not appear to have been generated by the compression utility.</p>
<div id="attachment_2164" style="width: 650px" class="wp-caption aligncenter"><a href="http://culturalanalytics.org/2019/02/could-fiction-have-an-information-history-statistical-probability-and-the-rise-of-the-novel/liddle_figure3/" rel="attachment wp-att-2164"><img aria-describedby="caption-attachment-2164" class="wp-image-2164" src="http://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure3.jpg" alt="" width="640" height="192" srcset="https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure3.jpg 3000w, https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure3-300x90.jpg 300w, https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure3-768x230.jpg 768w, https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure3-1024x307.jpg 1024w" sizes="(max-width: 640px) 100vw, 640px" /></a><p id="caption-attachment-2164" class="wp-caption-text">Figure 3. Post-compression sizes of 1MB text file slices of the .txtLAB collections of English, French, and German novels.</p></div>
<p>The second alternative explanation, that the result reflects some change in discourse within the wider culture, is more difficult to rule out. Corpus linguist Michaela Mahlberg, who studies represented speech in Dickens, has expressed frustration that corpus-to-corpus comparisons of dialogue in fiction with actual Victorian conversations are literally impossible to make, and also that in her experience peer reviewers expect them anyway.<sup><a href="#footnote_31_2155" id="identifier_31_2155" class="footnote-link footnote-identifier-link" title="In a 2016 lecture Mahlberg observes, “I get this a lot. When you send an article to a journal, then these reviewers give you these comments, ‘This is quite a nice idea, but actually you really need to compare this to a real spoken corpus,’ and like, that’s a very good idea, can someone show me a real spoken corpus of nineteenth-century language and I’ll happily do this.” Michaela Mahlberg, “Corpus linguistics and the challenges of close and distant reading,” 11th Sinclair Open Lecture, University of Birmingham, 20 Jun. 2016. Youtube, time index 20:40-21:07.">32</a></sup> Fortunately, nineteenth-century Britain does have one body of continuously "captured" public discourse, which even if far from ideal at least provides a body of single-genre comparison text created over the same period as nineteenth-century fiction. From the 1780s onward London morning newspapers posted teams of reporters in Parliament, Charles Dickens briefly among them, to work in shifts to take notes and write out textual replicas of the debates (in compressed third-person for minor Members, "full" first-person for party leaders) to pass to newspaper compositors for immediate publication in the next morning's newspaper.<sup><a href="#footnote_32_2155" id="identifier_32_2155" class="footnote-link footnote-identifier-link" title="An authoritative and readable overview of this system and its history is John Vice and Stephen Farrell, The History of Hansard (London: House of Lords Hansard and the House of Lords Library, 2017).">33</a></sup> These on-deadline transcripts were collected, edited, and republished as a quasi-official public record in <em>Hansard's Debates</em>, the nineteenth-century series of which are now available on the web.<sup><a href="#footnote_33_2155" id="identifier_33_2155" class="footnote-link footnote-identifier-link" title="See &quot;Hansard Archive (debates from 1803).&quot;">34</a></sup> If British novels in this era became denser only because of a culture-wide movement toward more information-dense discourses, the compressibility ratios of parliamentary transcripts over the same period might reflect the same trend. To keep the comparison fair, the calculation was limited to the period from 1803 until 1878, the year <em>Hansard </em>began to use a reportorial staff of its own in addition to collecting and collating the newspaper reports.<sup><a href="#footnote_34_2155" id="identifier_34_2155" class="footnote-link footnote-identifier-link" title="Some other manipulations of this data were necessary and should be noted. The text files are kept at http://www.hansard-archive.parliament.uk/, but the index to sittings per volume is at https://api.parliament.uk/historic-hansard/volumes/index.html. The two resources are not perfectly aligned; 40 volumes of Hansard are listed in the index but not present in the zipped files, while 24 volumes had a zipped text file but did not appear in the index. In the data set these volumes are simply omitted. Another five volumes had both index entries and a file but the dates of the sittings were listed as &quot;unknown.&quot; For these I inserted dates between those of the volumes that came before and after, as follows: s1v9 (15-8-1807 to 20-1-1808), s1v13 (8-3-1809 to 10-4-1809), s1v21 (25-7-1811 to 16-3-1812), s1v24 (5-5-1812 to 3-11-1813), s3v108 (2-8-1849 to 25-2-1850). Finally, the Hansard files download heavily laden with xml codes; to make sure compression was being done mostly on the original report texts these codes were removed using the text editor Notepad++.">35</a></sup> The trace this experiment produced is visualized in Figure 4 below.</p>
<div id="attachment_2163" style="width: 650px" class="wp-caption aligncenter"><a href="http://culturalanalytics.org/2019/02/could-fiction-have-an-information-history-statistical-probability-and-the-rise-of-the-novel/liddle_figure4/" rel="attachment wp-att-2163"><img aria-describedby="caption-attachment-2163" class="wp-image-2163" src="http://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure4.jpg" alt="" width="640" height="256" srcset="https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure4.jpg 3000w, https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure4-300x120.jpg 300w, https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure4-768x307.jpg 768w, https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure4-1024x410.jpg 1024w" sizes="(max-width: 640px) 100vw, 640px" /></a><p id="caption-attachment-2163" class="wp-caption-text">Figure 4. Compressibility of <em>Hansard</em> parliamentary debate transcript text files from 1803-1878. Source of data: Hansard.</p></div>
<p>A visible trend<span class="emdash">—</span>unexpectedly more down than up this time<span class="emdash">—</span>in the information density of Hansard-collected newspaper reports over the first three quarters of the nineteenth century seems to appear, but statistical significance was assessed anyway using the R Kendall package. The result was a negative trend (tau = -0.166) in the information density of debate transcript texts, to a strong degree of statistical significance (2-sided pvalue = 0.000040773). Over the period English fiction was apparently getting more information-dense, Parliamentary discourse, at least as newspapers reproduced it, was getting a bit less so.</p>
<p>How can this result be explained? One potentially important factor is suggested by the changing volume of parliamentary discourse transcript the teams of newspaper reporters were expected to produce for each parliamentary sitting, shown in Figure 5 below, which increased steadily, substantially, and significantly over the period (tau = 0.543, 2-sided pvalue = 2.22e-16). A weak but significant inverse correlation (cor = -0.1626734, p-value = 0.006967) appears to hold between the amount of transcript reporters were required to submit for each sitting and the information density of the resulting transcript. In other words, as the total volume of language the reporters had to produce to the same daily deadline for a given evening of speeches increased, the information density of the text those reporters produced decreased.<sup><a href="#footnote_35_2155" id="identifier_35_2155" class="footnote-link footnote-identifier-link" title="Interestingly, at least one veteran newspaper reporter of this era appears to have hypothesized a necessary tradeoff between time pressure and textual density. Charles Ross, who reported Parliamentary debates for The Times from 1820 until 1883 and was its Chief of Reporters from the mid-1850s, told a Parliamentary subcommittee in July 1878 that deadline pressures had made reporting “too much of a mechanical art at present; giving the mere words.” Ross preferred that “there should be compression instead of expansion; you want a man’s opinions, not his words,” but told the subcommittee that “under the existing system it cannot well be done.” Great Britain, Parliament, Reports from Committees, 1878. Vol. 8. “Report from the Select Committee on Parliamentary Reporting,” 39.">36</a></sup></p>
<div id="attachment_2162" style="width: 650px" class="wp-caption aligncenter"><a href="http://culturalanalytics.org/2019/02/could-fiction-have-an-information-history-statistical-probability-and-the-rise-of-the-novel/liddle_figure5/" rel="attachment wp-att-2162"><img aria-describedby="caption-attachment-2162" class="wp-image-2162" src="http://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure5.jpg" alt="" width="640" height="256" srcset="https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure5.jpg 3000w, https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure5-300x120.jpg 300w, https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure5-768x307.jpg 768w, https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure5-1024x410.jpg 1024w" sizes="(max-width: 640px) 100vw, 640px" /></a><p id="caption-attachment-2162" class="wp-caption-text">Figure 5. Amount of transcript text published per Parliamentary sitting in Hansard, 1803-1888, in average amount (in KB) of uncompressed text per event. Source of data: Hansard.</p></div>
<p>The <em>Hansard </em>results do not appear to support the alternative hypothesis that novels in Britain might have become more information-dense over the nineteenth century simply because discourse or language use in the wider culture was doing so. If anything, novels became more information-dense in this period despite a slightly contrary trend in public discourse, although one experiment on a corpus created for other purposes is not strong evidence that a wider trend existed at all. The results do suggest, however, that pressure to produce more text in the same amount of time acts to lower the information density of the text thus produced.</p>
<p>Could the same cause have produced the same effect on literary composition, and could the relative compressibility of novels by individual authors over time perhaps even reveal information histories within individual artistic careers? With a trend toward greater information density in nineteenth-century English and French novels not disconfirmed by the Piper and Parliamentary experiments, it seemed important to ask whether this effect might also be visible at the level of the author. A productive author with a high total count or <em>n </em>of text files seemed the best way to produce meaningful results. Anthony Trollope published nearly 50 novels, in addition to short story collections and several nonfiction works, over a long career that also happens to have been divided into two quite different approaches to composition. Until 1860 Trollope's productivity as a novelist, though impressive by modern and most Victorian standards, was slow by comparison with his own later pace. The son of popular novelist Frances Trollope, Anthony Trollope wrote and published novels in this early era at intervals of a year to a year and a half or more, winning little critical or financial success and keeping his primary professional focus on his work as a Post Office civil servant. Once he was chosen by William Makepeace Thackeray to become a regular contributor of serial fiction to the <em>Cornhill Magazine</em>, however, Trollope became one of the most productive major writers of the Victorian era, rapidly completing multiple large novels per year for most of the rest of his life using the disciplined daily methods and word count targets described in his <em>Autobiography </em>(written by 1878, published 1883).<sup><a href="#footnote_36_2155" id="identifier_36_2155" class="footnote-link footnote-identifier-link" title="Anthony Trollope, An Autobiography (Oxford and New York: Oxford University Press, 1980).">37</a></sup></p>
<p>To see if Trollope's shift to a faster textual production pace had any effect on the statistical information density of the resulting work, all 47 of his novels were downloaded from Project Gutenberg and the texts of each file stripped<span class="emdash">—</span>for this purpose only<span class="emdash">—</span>of editorial apparatus and Project Gutenberg legal language.<sup><a href="#footnote_37_2155" id="identifier_37_2155" class="footnote-link footnote-identifier-link" title="I assure Project Gutenberg’s legal representatives that the truncated text files were created for research purposes only and have not been shared.">38</a></sup> Originally a Python routine was written to cycle through the text files, compress each, calculate the degree of compression, and plot the result with Matplotlib.<sup><a href="#footnote_38_2155" id="identifier_38_2155" class="footnote-link footnote-identifier-link" title="This part of the project would not have originally been possible without technical help and advice provided through a 2016 Advanced Collaborative Support Grant from the HathiTrust Research Center. Grateful thanks are due to Peter Organisiak, Ryan Dubnicek, Eleanor Dickson Koehl, and Nandana Nallapu.">39</a></sup>To keep methods and comparisons consistent in this paper, the experiment was rerun with files compressed with WinRAR and visualized in R to produce Figure 6. Once more, the null hypothesis was that the compressibility of novels by a single author should tend to be stable over time, consistent with the finding of numerous digital humanists that works by individual authors display a distinctive career-long "signal" of largely unconscious patterns of usage, especially in rates of use of "little words."<sup><a href="#footnote_39_2155" id="identifier_39_2155" class="footnote-link footnote-identifier-link" title="This “little words” analysis was pioneered by J. F. Burrows, Computation into Criticism: A Study in Jane Austen's Novels and an Experiment in Method (Oxford: Oxford University Press, 1987). For modern discussion and application of stylistic analysis using modern statistical software see Matthew Jockers, Macroanalysis: Digital Methods and Literary History (Chicago: University of Illinois Press, 2013).">40</a></sup></p>
<p>Figure 6 seems to show that Trollope's change to more rapid textual production in 1860 did coincide with a significant drop in the information density of his fiction (overall tau -0.338, 2-sided pvalue = 0.0008437). The early novels written over longer periods are so much <em>less </em>compressible than the later ones, in fact, that they occupy an entirely different space in the visualization, hardly overlapping the later fiction at all. Quantitative stylistics practitioners might be surprised by the ease with which a test for file compressibility could apparently enable a scholar to identify a text as early or late Trollope, to a high degree of accuracy, simply by zipping it. A compressibility ratio of .361 seems to be the cutoff: before 1860 Trollope was never below this, while for the remaining 35 novels of his career he rose above it only for <em>The Way We Live Now </em>and <em>Harry Heathcote of Gangoil </em>(both 1873), the science fiction experiment <em>The Fixed Period </em>(1881), and <em>The Landleaguers </em>(1882). As with the newspaper reporters who created the Hansard transcripts, stronger pressures (in this case self-imposed) to produce more text in the same amount of time seems to have covaried with a significant decrease in the information density of the text produced.</p>
<div id="attachment_2161" style="width: 650px" class="wp-caption aligncenter"><a href="http://culturalanalytics.org/2019/02/could-fiction-have-an-information-history-statistical-probability-and-the-rise-of-the-novel/liddle_figure6/" rel="attachment wp-att-2161"><img aria-describedby="caption-attachment-2161" class="wp-image-2161" src="http://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure6.jpg" alt="" width="640" height="256" srcset="https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure6.jpg 3000w, https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure6-300x120.jpg 300w, https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure6-768x307.jpg 768w, https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure6-1024x410.jpg 1024w" sizes="(max-width: 640px) 100vw, 640px" /></a><p id="caption-attachment-2161" class="wp-caption-text">Figure 6.  Compressibility of plain text files of Anthony Trollope's 47 novels by date. Source of files: Project Gutenberg.</p></div>
<p>At the same time, however, the Stanford hypothesis that the <em>canonicity </em>of a work of fiction is the variable most directly related to its redundancy is not well supported by the Trollope results. Popular early novels such as <em>The Warden </em>(1855) do score high for information density, but so do the almost unread early works that preceded it, such as <em>The Kellys and the O'Kellys </em>(1848). The only factor with a consistent relationship to information density is the period in Trollope's career in which a work was written, and by implication the speed at which it was written.</p>
<p>To check this result against a second nineteenth century novelist with a large <em>oeuvre</em>, the same procedure was used on Sir Walter Scott, whose 26 full-length novels are all available from Project Gutenberg and were treated the same way. The short stories and nonfiction were excluded, as in Trollope's case, but Scott's long fictions also included the narrative poems <em>Lay of the Last Minstrel </em>(1805)<em>, Marmion </em>(1808)<em>, Lady of the Lake </em>(1810)<em>, </em>and <em>Rokeby </em>(1813), which Scott's biographers agree were written with more care and over much longer periods than his novels. In fact, as numerous primary and secondary sources attest, Scott turned to novel writing mid-career with the deliberate intention of working in a genre he could produce more quickly than long poetry. The poems were unavailable from Project Gutenberg, so<span class="emdash">—</span>at some risk of introducing a confounding variable<span class="emdash">—</span>they were downloaded as plain e-texts from the University of Adelaide's eBooks@Adelaide, cleaned of extraneous code, and added to the set of Scott files as plain text UTF-8 files to match the file format of the fiction.<sup><a href="#footnote_40_2155" id="identifier_40_2155" class="footnote-link footnote-identifier-link" title="eBooks@Adelaide, University of Adelaide Library, https://ebooks.adelaide.edu.au">41</a></sup></p>
<p>Running the same compression routine on the Scott files produced the data shown in Figure 7, which seems to show that Scott's career as well as Trollope's had a mathematically detectable information history, and of just the sort we might expect. The long poems issued at three- to four-year intervals between 1805 and 1813 are compressible only to levels of .40 or more. The main series of Waverley novels, more quickly written and published beginning in 1814, are much more compressible, inhabiting the .38 to .39 range of the visualization. The final novels written after Scott had suffered several strokes, <em>Count Robert of Paris </em>and <em>Castle Dangerous</em><span class="emdash">—</span>generally considered his least valuable and successful<span class="emdash">—</span>drop below .38, indicating (though with only two data points) that his condition empirically altered Scott's writing in ways that track with reader experience. Any other conclusions about the career as a whole are harder to draw. A small statistically significant decline in information density over Scott's career appears even if only novels are measured and the poetry excluded (tau = -0.286, 2-sided pvalue = 0.042578), but if the data are truncated to exclude the post-stroke novels the statistical significance of any gradual density decline over the Waverley set disappears (tau = -.159, 2-sided pvalue = 0.28616).</p>
<p>These results make it more difficult to rule out a relationship between canonicity and information density in Scott's case than in Trollope's. Early well-received fictions such as <em>Waverley</em>, <em>Guy Mannering, </em>and <em>The Antiquary </em>do seem relatively less compressible than the other prose fictions, but some of Scott's most highly-regarded mid-career fictions, including <em>Heart of Midlothian</em>, also score lower in compressibility. Scott may simply have been a fairly consistent fictional coder during the main part of his career, or this experiment may point to limitations to the sensitivity of the compressibility ratio test. It certainly underscores the importance of using it on larger textual oeuvres.</p>
<div id="attachment_2160" style="width: 650px" class="wp-caption aligncenter"><a href="http://culturalanalytics.org/2019/02/could-fiction-have-an-information-history-statistical-probability-and-the-rise-of-the-novel/liddle_figure7/" rel="attachment wp-att-2160"><img aria-describedby="caption-attachment-2160" class="wp-image-2160" src="http://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure7.jpg" alt="" width="640" height="256" srcset="https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure7.jpg 3000w, https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure7-300x120.jpg 300w, https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure7-768x307.jpg 768w, https://culturalanalytics.org/wp-content/uploads/2019/01/Liddle_Figure7-1024x410.jpg 1024w" sizes="(max-width: 640px) 100vw, 640px" /></a><p id="caption-attachment-2160" class="wp-caption-text">Figure 7. Compressibility of plain text of four long narrative poems and 26 novels by Sir Walter Scott, by original publication date.</p></div>
<p> </p>
<h1 style="text-align: center;">Discussion: An information mathematics of fiction?</h1>
<p>If multiple experiments using different corpora suggest that national histories of the novel, and even the careers of individual novelists, may reflect information-theoretical pressures in a statistically significant way, literary historians may have to take seriously the hypothesis that mathematical information density has some relationship to literary forms and literary history. It may be necessary to assess the implications of this idea, and to plan further investigations to confirm or disconfirm these apparent effects.</p>
<p>As has been noted, literary scholars since the 1960s have been disinclined to believe--for what appeared excellent reasons<span class="emdash">—</span>that the information readers recognize in fiction might be the same kind of phenomenon, or have any of the same properties, as the electronic or digital "information" streamed over telegraph, telephone, or fiber optic cables, or stored in computer memory.<sup><a href="#footnote_41_2155" id="identifier_41_2155" class="footnote-link footnote-identifier-link" title="The fascinating story of how and why philosophers and humanists in the 1940s and 1950s briefly engaged and then disengaged Shannon's Information Theory is well told in Bernard Dionysius Geoghegan, &quot;From Information Theory to French Theory: Jakobson, Levi-Strauss, and the Cybernetic Apparatus,&quot; Critical Inquiry 38.1 (2011): 96-126.">42</a></sup>To most scholars such a parallel has seemed too unlikely even to need refuting, although in the early excitement generated in the mathematical and scientific communities of the 1950s and 1960s by the engineering success of Information Theory many took time to refute it anyway.<sup><a href="#footnote_42_2155" id="identifier_42_2155" class="footnote-link footnote-identifier-link" title="See, for one particularly convinced anti-Information Theory example, Carl H. Weaver and Garry L. Weaver, &quot;Information Theory and the Measurement of Meaning,&quot; Speech Monographs 32 (1965): 435-447.">43</a></sup> Some have continued to do so, as when John Gunders wrote in 2002 that "the redundancy that Shannon discusses has very little to do with the surprise with which a text may confront readers or their ability to anticipate and select the appropriate response."<sup><a href="#footnote_43_2155" id="identifier_43_2155" class="footnote-link footnote-identifier-link" title="John Gunders, &quot;Signal or Noise? Information Theory and the Novel,&quot; Double Dialogues 3 (Summer 2002).">44</a></sup> Over the same period a small group of humanists and critics, more strangely, have invoked Shannon in service of critical theory, attracted by the quasi-paradox that for engineers "noise" is the opposite of signal, but also the most mathematically information-dense kind of transmission, since white noise is completely random and unpatterned. This group includes Michel Serres, Jacques Attali, and Henri Atlan in France, Freidrich Kittler in Germany, and William Paulson and Philipp Schweigerson in the U.S. None of these scholars actually adopt Shannon's mathematical quantification of noise along with his definition of it, however, and some go out of their way to reject it.<sup><a href="#footnote_44_2155" id="identifier_44_2155" class="footnote-link footnote-identifier-link" title="For good explications of this position see William R. Paulson, The Noise of Culture: Literary Texts in a World of Information (Ithaca and London: Cornell University Press, 1988), and Philipp Schweighauser's entry on &quot;Information Theory&quot; in Bruce Clark and Manuela Rossini, eds, The Routledge Companion to Literature and Science (New York: Routledge, 2012), 145-156.">45</a></sup> Paulson writes that, "[W]e are not dealing with the direct application of a working scientific theory to a new empirical domain: the literary text cannot simply be placed in the position of electronic signals and analyzed quantitatively."<sup><a href="#footnote_45_2155" id="identifier_45_2155" class="footnote-link footnote-identifier-link" title="Paulson, 65.">46</a></sup> David Letzler goes further, classing the whole idea of applying information mathematics to literature with the poststructuralist misuses of science that inspired Alan Sokal's <em>Social Text </em>hoax.<sup><a href="#footnote_46_2155" id="identifier_46_2155" class="footnote-link footnote-identifier-link" title="David Letzler, &quot;Crossed-Up Disciplinarity: What Norbert Wiener, Thomas Pynchon, and William Gaddis Got Wrong About Entropy and Literature,&quot; Contemporary Literature 56.1 (2015), 52.">47</a></sup></p>
<p>As recently as the early 2000s, this opinion that information theory was for computer engineers alone was widely shared in other disciplines as well. To that date even most linguistics scholars<span class="emdash">—</span>in many cases approaching the idea of linguistic information through Chomskyan assumptions about generative grammar<span class="emdash">—</span>left the mathematical model of information untouched, even though actual engineers, including computer scientists such as Peter Norvig, who was to become a director of research at Google, were already successfully applying statistical modeling based on information theory to linguistic processing in information retrieval, and beginning to achieve the real-world results already discussed.<sup><a href="#footnote_47_2155" id="identifier_47_2155" class="footnote-link footnote-identifier-link" title="Norvig's readable essay evaluating Chomsky's continuing claims (as recently as 2011) that human language cannot usefully be statistically modeled is, &quot;On Chomsky and the Two Cultures of Statistical Learning,&quot; reproduced at Norvig's website, http://norvig.com/chomsky.html">48</a></sup></p>
<p>In the past ten years, however, researchers' assumptions across a range of related disciplines about the value of information theory for investigating human language use has begun to alter. In experimental studies involving both corpus analysis and laboratory work with human subjects, researchers at MIT's Department of Brain and Cognitive Sciences, the University of Barcelona's Complexity and Cognitive Linguistics Lab, and other institutions have independently produced evidence that information theory may usefully model some aspects of how minds produce and exchange language. In 2011 psychologist Steven T. Piantadosi was able to show with corpus research that statistical information content accurately predicts word lengths in natural language, and in a separate study the next year that information-theoretical ambiguity in natural language improves its information efficiency.<sup><a href="#footnote_48_2155" id="identifier_48_2155" class="footnote-link footnote-identifier-link" title="Steven T. Piantadosi, Harry Tily, and Edward Gibson, &quot;Word lengths are optimized for efficient communication,&quot; PNAS 108.9 (2011); and Steven T. Piantadosi, Harry Tily, and Edward Gibson, &quot;The Communicative Function of Ambiguity in Language,&quot; Cognition 122 (2012): 280-291.">49</a></sup> In 2013 Kyle Mahowald et al. showed that speakers demonstrate information efficiency in making word choices.<sup><a href="#footnote_49_2155" id="identifier_49_2155" class="footnote-link footnote-identifier-link" title="Kyle Mahowald, Evelina Fedorenko, Steven T. Piantadosi, and Edward Gibson, &quot;Info/information theory: Speakers choose shorter words in predictive contexts,&quot; Cognition 126 (2013): 313-318.">50</a></sup> Yang Xu and David Reitter of Pennsylvania State University recently proposed a formal "information-theoretic view of dialogue" after demonstrating that dialogue partners mutually adapt the information density of their speech as their discourse develops.<sup><a href="#footnote_50_2155" id="identifier_50_2155" class="footnote-link footnote-identifier-link" title="Yang Xu and David Reiter, &quot;Information Density Converges in Dialogue: Towards an Information-Theoretic Model,&quot; Cognition 170 (2018), 147-163.">51</a></sup> Laboratory studies of reading have also offered evidence that information mechanics may partly structure the experience of literary narrative. Rolf Zwann found that subjects given a passage of narrative and told that it is fiction read significantly more slowly than subjects told the same passage is a piece of journalism, suggesting that readers expect<span class="emdash">—</span>have learned by experience to expect?<span class="emdash">—</span>greater information density in fiction than in nonfiction, and slow down to better process it.<sup><a href="#footnote_51_2155" id="identifier_51_2155" class="footnote-link footnote-identifier-link" title="Rolf A. Zwaan, &quot;Effect of Genre Expectations on Text Comprehension,&quot; Journal of Experimental Psychology: Learning, Memory, and Cognition 20 (1994): 920-933.">52</a></sup></p>
<p>Even other recent work in the literary digital humanities, revisited from this perspective, may support the idea that humanist information has more in common with statistical information than we have believed. Franco Moretti's 2007 <em>Critical Inquiry </em>article "Style, Inc." showed quantitatively that titles of novels published in Britain in the eighteenth and early nineteenth centuries became shorter over time, but also with close reading that the new short title structures invented during this process conveyed greater significance in fewer words.<sup><a href="#footnote_52_2155" id="identifier_52_2155" class="footnote-link footnote-identifier-link" title="Franco Moretti, &quot;Style, Inc. Reflections on Seven Thousand Titles (British Novels, 1740-1850),” Critical Inquiry 36 (2009): 134-158.">53</a></sup> Andrew Piper, in "Fictionality," makes sophisticated use of the LWIC tool to discover a statistically significant rise over the nineteenth century of language associated with <em>perception </em>and with <em>doubt </em>in English-language fiction.<sup><a href="#footnote_53_2155" id="identifier_53_2155" class="footnote-link footnote-identifier-link" title="Andrew Piper, &quot;Fictionality,&quot; CA: Journal of Cultural Analytics 20 Dec. 2016. Article DOI: 10.22148/16.011, Dataverse DOI:  10.7910/DVN/5WKTZV">54</a></sup> Ryan Heuser and Long Le-Khac used topic modeling to discover a century-long trend in long fiction away from generalization and abstraction toward specific and concrete language. In all these cases the authors ascribe causation to non-information-theoretical factors: Moretti attributed the change he observed to the workings of natural-selection-style evolution in a market, while Piper, Heuser, and Le-Khac proposed historical and cultural trends. Their findings are consistent with gradually improving information performance as well, however, because the language forms they find increasing over time are those that better concentrate the qualities information theory measures.</p>
<p>These research results suggest that the belief of literary scholars that information in texts belongs in a different epistemological category than information in machines<span class="emdash">—</span>that it obeys different laws and is unreachable by empirical inquiry<span class="emdash">—</span>may soon be unsustainable. Even our professional experiences as scholarly writers and teachers of writing may have been showing us a different set of forces at work. Like Victorian parliamentary reporters, many of us have had experience that when we write long texts to short deadlines the results are likely to be wordy and redundant. Some have probably observed that in enterprises where language must be used under constraints, or where accuracy is especially important, practitioners expand or contract their discourse to match their system's needs, developing techniques to balance production time, language volume, and information content. The ten-codes used by police, fire, and rescue services, the abbreviations and initialisms in military orders and hospital emergency rooms, the clarity of flight traffic instructions, the clipped codes exchanged by restaurant wait staff and line cooks, are all linguistic responses to systemically constrained information exchange. Long-form as well as short-form professional genres are altered by such pressures. Generations of journalism students have been taught a (somewhat exaggerated) story of how news organizations invented the "inverted pyramid" to concentrate important information at the beginning of a document when telegraph service became unreliable during the American Civil War. They are less often given the example of Victorian freelance "penny-a-line" reporters, whose reimbursement was determined by the number of lines of their submissions the newspaper printed, and who responded to those incentives by coding ever-smaller quantities of newsworthy information into ever-larger quantities of verbiage.</p>
<p>If an information-theoretical mechanism for both Algee-Hewitt et al.'s result and the compression ratio results described here are confirmed, literary historians might need to find new kinds of stories to tell about the historical forces that helped shape the development of narrative fiction. To explain the increasing information density of English and French novels over the nineteenth century, for example, we might consider a hypothesis that from the mid-eighteenth century onward novelists faced a technical challenge inherent in the ideal of a prose fiction at once realistic and artistic. Poetry concentrates signification using specialized and aesthetically optimized language codes, but realist fiction has to use<span class="emdash">—</span>or seem to use<span class="emdash">—</span>the same narrative and speech genres as contemporary social interaction. The challenge this poses for artists is that social speech genres are mostly low-density, marked by high redundancy and the use of premade linguistic formulae (Douglas Biber and Susan Conrad's "lexical bundles") that render spoken language easy to produce in real time.<sup><a href="#footnote_54_2155" id="identifier_54_2155" class="footnote-link footnote-identifier-link" title="See for example D. Biber, S. Conrad, and V. Cortes, &quot;If you look at ... : Lexical bundles in university teaching and textbooks,&quot; Applied Linguistics 25 (2004): 371-405.">55</a></sup> Novelists trying to construct high-information-density artistic texts out of low-information-density discourse genres may never have been in the enviable position imagined by critics such as Pavel, free to confabulate without constraint, and more in the one imagined by Vladimir Nabokov, required to create art by assembling "dazzling combinations of drab parts."<sup><a href="#footnote_55_2155" id="identifier_55_2155" class="footnote-link footnote-identifier-link" title="Vladimir Nabokov, Nicolai Gogol (New York: New Directions, 1961), 56.">56</a></sup> Raymond Chandler described the difficulty of writing dialogue in Hollywood at the mid-20th century: "the challenge of screenwriting is to say much in little and then take half of that little out and still preserve an effect of leisure and natural movement." The only solution to such problems, Chandler wrote, was "experiment and elimination."<sup><a href="#footnote_56_2155" id="identifier_56_2155" class="footnote-link footnote-identifier-link" title="Raymond Chandler, Raymond Chandler Speaking, ed. Dorothy Gardiner and Kathrine Sorley Walker (Berkeley and Los Angeles: University of California Press, 1997), 119.">57</a></sup> The historical rise in the average information density of French and English fiction seen in Figures 1 and 3 may be a trace left by novelists addressing the complex information problem of artistic realist fiction by experiment and elimination also, gradually accumulating an expanded repertoire of more effective coding tools by successive variation in their discourse practices, and by copying the successful variations of peers.</p>
<p>An emerging information history of fiction might also help us make connections among a range of literary phenomena and critical theories we currently treat as separate. In communications engineering, reducing statistical redundancy is only one way to increase the information performance of a channel. Coders can also increase the local uncertainty of the individual components of the message that remain; in the case of fiction, this might mean making uncertainty itself a central topical interest in fiction, as Piper seems to have found, or reducing the average generality of language used in favor of more concrete detail, as Heuser and Le-Khac showed. A still more effective solution to information bottlenecks in many technical communication systems is to "multiplex" the channel by finding ways to send more than one message simultaneously. Edison's "Quadruplex" telegraph used phase modulation to send one message and amplitude modulation to send another across the same wire at the same time; novelistic equivalents may include the stylistic inventions literary scholars know as Free Indirect Discourse, Mikhail Bakhtin's heteroglossia and polyvocality, and Lisa Zunshine's application to fiction of Theory of Mind,<sup><a href="#footnote_57_2155" id="identifier_57_2155" class="footnote-link footnote-identifier-link" title="M.M. Bakhtin, The Dialogic Imagination: Four Essays (Austin: University of Texas Press, 1981); Lisa Zunshine, Why We Read Fiction: Theory of Mind and the Novel (Columbus: Ohio State University Press, 2006).">58</a></sup> which may not be mutually unrelated phenomena. Novelists under information coding pressure may value and seek ways to make single lines of text do double duty, and "multiplexing" novelistic discourse by simultaneously conveying a narrator's perspective and a character's, one character's thoughts and another's, or a character's own perspective together with its ironization, do this double duty effectively.</p>
<p>Humanists are within their disciplinary rights to require rigorous evidence before accepting claims that empirical measures can apply to human expression, or that mathematical constraints might have helped shape imaginative language. Even for the most cherished and traditional purposes of literary history and criticism, however, techniques that allow us to trace an information history for fiction may be valuable. If literary scholars are to appreciate as fully as possible the craft, cognitive powers, and artistry required by great fiction, we cannot afford to ignore any of the constraints under which<span class="emdash">—</span>and against which<span class="emdash">—</span>novelists have had to innovate and succeed.</p>
<p><a href="http://creativecommons.org/licenses/by/4.0/" rel="license"><img src="https://i.creativecommons.org/l/by/4.0/88x31.png" alt="Creative Commons License" /></a><br />
Unless otherwise specified, all work in this journal is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>.</p>
 
					
					
					
										</div>
	</div>
<div id='footnote_div'><ol class="footnotes"><li id="footnote_0_2155" class="footnote">The author is grateful to the editor and reviewers of CA for generous feedback on earlier drafts of this article, which led me to ask new questions, run new tests, and seek out resources I had entirely missed. Everything still missing in the current version is my sole responsibility. Some parts of the work described here were supported by a 2010 NEH Summer Stipend, others by a 2016 Advanced Collaborative Support Grant from the HathiTrust Research Center. [<a href="#identifier_0_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_1_2155" class="footnote">Katherine Bode, <em>A World of Fiction: Digital Collections and the Future of Literary History </em>(Ann Arbor: University of Michigan Press, 2018), 96. [<a href="#identifier_1_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_2_2155" class="footnote">Thomas G. Pavel, <em>Fictional Worlds </em>(Cambridge: Harvard University Press, 1986), 2. [<a href="#identifier_2_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_3_2155" class="footnote">Mark Algee-Hewitt, Sarah Allison, Marissa Gemma, Ryan Heuser, Franco Moretti, Hannah Walser, "<a href="https://litlab.stanford.edu/LiteraryLabPamphlet11.pdf">Canon/Archive: Large-scale Dynamics in the Literary Field</a>," <em>Stanford Literary Lab Pamphlet 11</em>. Jan. 2016. [<a href="#identifier_3_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_4_2155" class="footnote">For a description of the collection see "<a href="http://collections.chadwyck.co.uk/marketing/products/about_ilc.jsp?collection=ncf">About Nineteenth-Century Fiction</a>," <em>Chadwyck-Healey Literature Collections</em>. c. 1996-2016. [<a href="#identifier_4_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_5_2155" class="footnote">The mathematical Information Theory invoked and used by Algee-Hewitt et al. is not a standard area of undergraduate mathematics, but a number of excellent introductions and guides have been written. For a brief and readable overview I recommend the background section of M. B. Plenio and V. Vitelli, "The physics of forgetting: Landauer's erasure principle and information theory," <em>Contemporary Physics </em>42.1 (2001): 25-60, available as a PDF from the <a href="http://www3.imperial.ac.uk/pls/portallive/docs/1/55905.PDF">Imperial College London website</a>. A longer introduction aimed at the generalist is John R. Pierce, <em>An Introduction to Information Theory: Symbols, Signals, and Noise</em>, rev. ed. (New York: Dover, 1980), which was reviewed in manuscript by Claude Shannon. The standard textbook for Information Theory at the graduate level is Thomas M. Cover and Joy A. Thomas, <em>Elements of Information Theory </em>(New York and Chichester: John Wiley, 1991). To go back to the source see Claude E. Shannon, "<a href="http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">A Mathematical Theory of Communication</a>," <em>The Bell System Technical Journal </em>27 (July and October 1948): 379-423, 623-656, a reprint of which is available on the website of the Mathematics department at Harvard. [<a href="#identifier_5_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_6_2155" class="footnote">Algee-Hewitt et al., 6. [<a href="#identifier_6_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_7_2155" class="footnote">Algee-Hewitt et al., 6. [<a href="#identifier_7_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_8_2155" class="footnote">Algee-Hewitt et al., 7. [<a href="#identifier_8_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_9_2155" class="footnote">Noam Chomsky, "Three Models for the Description of Language," <em>IRE Transactions on Information Theory </em>2.3 (1956), 116.  [<a href="#identifier_9_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_10_2155" class="footnote">Cherry told an interviewer in the 1970s that Information Theory "doesn't help us" much to understand human communication [....] It was a blind alley." See Carol Wilder, "A Conversation with Colin Cherry," <em>Human Communication Research </em>3.4 (1977), 356. [<a href="#identifier_10_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_11_2155" class="footnote">Shannon's position on this issue is usually overstated, however. While he did write that the semantic aspects of communication were irrelevant to the engineering aspects, he never claimed the engineering aspects were equally irrelevant to semantic communication. The metaphor that statistical information is "divorced" from meaning is therefore misapplied. Divorces make a mutual separation; Shannon's claim of separation went only one direction. [<a href="#identifier_11_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_12_2155" class="footnote">"<a href="http://www.oed.com/view/Entry/95568">information, n.</a>" OED Online. June 2017. Oxford University Press. (accessed December 21, 2017). [<a href="#identifier_12_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_13_2155" class="footnote">N. Katherine Hayles, "Cognition Everywhere: The Rise of the Cognitive Nonconscious and the Costs of Consciousness," <em>New Literary History </em>45 (2014), 216-217. [<a href="#identifier_13_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_14_2155" class="footnote">Neither Pamphlet 11 nor the Literary Lab website share the original results, list of archival texts, or the code of the algorithm used to generate their redundancy figures. Like the other Stanford pamphlets, the work was also not subjected to peer review. For the reasoning behind the latter decision see Franco Moretti, "<a href="https://litlab.stanford.edu/LiteraryLabPamphlet12.pdf" target="_blank" rel="noopener noreferrer" data-auth="NotApplicable">Literature, Measured</a>," Stanford Literary Lab Pamphlet 12, April 2016. [<a href="#identifier_14_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_15_2155" class="footnote">An excellent study of how the development of digital and non-digital communication networks were influenced by the same patterns and forces is Gerard J. Holzmann and Björn Pehrson, <em>The Early History of Data Networks </em>(Washington and Brussels: IEEE Computer Society Press, 1995). [<a href="#identifier_15_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_16_2155" class="footnote">See for example Richard Menke, <em>Telegraphic Realism: Victorian Fiction and Other Information Systems </em>(Stanford: Stanford UP, 2008); Cara Murray, <em>Victorian Narrative Technologies in the Middle East </em>(New York: Routledge, 2008); and Tony Jackson, <em>The Technology of the Novel: Writing and Narrative in British Fiction </em>(Baltimore: Johns Hopkins UP, 2009). [<a href="#identifier_16_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_17_2155" class="footnote">Kenneth W. Church, "Statistical Models for Natural Language Processing," <em>Oxford Handbook of Computational Linguistics 2<sup>nd </sup>edition, </em>ed. Ruslan Mitkov. Online publication date: Aug. 2016. DOI: 10.1093/oxfordhb/9780199573691.013.54. [<a href="#identifier_17_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_18_2155" class="footnote">See for example Max Bane, "Quantifying and Measuring Morphological Complexity," in <em>Proceedings of the 26th West Coast Conference on Formal Linguistics</em>, ed. Charles B. Chang and Hannah J. Haynie (Somerville, MA: Cascadilla Proceedings Project, 2008), 69-76. Available:<a href="http://www.lingref.com"> www.lingref.com</a>, document #1657. Patrick Juola, "Assessing linguistic complexity," is in Matti Miestamo, Kaius Sinnemaki and Fred Karlsson, eds., <em>Language Complexity: Typology, Contact, Change </em>(Amsterdam/Philadelphia: Benjamins, 2008), 89-108. The compressibility of 21 different translations of the EU Constitution was tested by Markus Sadeniemi, Kimmo Kettunen, Tiina Lindh-Knuutila, and Timo Honkela in "Complexity of European Union Languages: A Comparative Approach," <em>Journal of Quantitative Linguistics </em>15.2 (2008): 185-211. [<a href="#identifier_18_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_19_2155" class="footnote">See Ming Li and Paul Vitányi, <em>An Introduction to Kolmogorov Complexity and its Applications</em>, 2<sup>nd </sup>ed. (New York and Berlin, Springer, 1997). [<a href="#identifier_19_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_20_2155" class="footnote">"Fortunately, it can be shown that all reasonable choices of programming languages lead to quantification of the amount of 'absolute' information in individual objects that is invariant up to an additive constant. We call this quantity the 'Kolmogorov complexity' of the object. If an object contains regularities, then it has a shorter description than itself. We call such an object 'compressible.'" Li and Vitányi, v. [<a href="#identifier_20_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_21_2155" class="footnote">Ehret and Szmrecsanyi use the utility gzip, while Sadeniemi et al. use Bzip2. Juola, who has used both, reports they produce similar results; see Juola, 102n3. [<a href="#identifier_21_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_22_2155" class="footnote">Juola, "Assessing Linguistic Complexity," 92. [<a href="#identifier_22_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_23_2155" class="footnote">Katharina Ehret and Benedikt Szmrecsany, "An Information-theoretic approach to assess linguistic complexity," in R. Baechler and G. Seiler, eds, <em>Complexity, isolation, and variation </em>(Berlin/Boston, MA: De Gruyter, 2016), 74. [<a href="#identifier_23_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_24_2155" class="footnote">Katharina Ehret and Benedikt Szmrecsanyi, "Compressing learner language: An information-theoretic measure of complexity in SLA production data," in <em>Second Language Research </em>(2016): 10. [<a href="#identifier_24_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_25_2155" class="footnote">Ehret and Szmrecsanyi, "Compressing Learner Language," 19. [<a href="#identifier_25_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_26_2155" class="footnote">Andrew Piper, <a href="https://dx.doi.org/10.6084/m9.figshare.2062002.v3">txtlab Multilingual Novels</a>, 2016. Figshare. [<a href="#identifier_26_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_27_2155" class="footnote">R: A Language and Environment for Statistical Computing, R Core Team, R Foundation for Statistical Computing, Vienna, Austria, 2018. <a href="https://www.r-project.org/">https://www.R-project.org</a>; and H. Wickham, "ggplot2: Elegant Graphics for Data Analysis" (New York: Springer-Verlag, 2016). [<a href="#identifier_27_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_28_2155" class="footnote">A.I. McLeod, "<a href="https://cran.r-project.org/web/packages/Kendall/Kendall.pdf">Package 'Kendall</a>.'" 16 May 2011. [<a href="#identifier_28_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_29_2155" class="footnote">See Ryan Heuser and Long Le-Khac, "<a href="http://litlab.stanford.edu/LiteraryLabPamphlet4.pdf">A Quantitative Literary History of 2,958 Nineteenth-Century British Novels: The Semantic Cohort Method</a>," Literary Lab Pamphlet 4, May 2012. Alan Liu's endorsement of their approach appears in "The Meaning of the Digital Humanities," <em>PMLA </em>128 (2013): 409-423. More recently Matt Erlin, in "Topic Modeling, Epistemology, and the English and German Novel," <em>Journal of Cultural Analytics </em>1 May 2017, Article DOI: <a href="https://doi.org/10.22148/16.014">10.22148/16.014</a>, Dataverse DOI: <a href="http://dx.doi.org/10.7910/DVN/3J38FX">10.7910/DVN/3J38FX</a>, interprets new research findings of his own as corroborating theirs. [<a href="#identifier_29_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_30_2155" class="footnote">Ehret and Szmrecsanyi, "Compressing Learner Language," 19. [<a href="#identifier_30_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_31_2155" class="footnote">In a 2016 lecture Mahlberg observes, "I get this a lot. When you send an article to a journal, then these reviewers give you these comments, 'This is quite a nice idea, but actually you really need to compare this to a real spoken corpus,' and like, that's a very good idea, can someone show me a real spoken corpus of nineteenth-century language and I'll happily do this." Michaela Mahlberg, "Corpus linguistics and the challenges of close and distant reading," 11<sup>th </sup>Sinclair Open Lecture, University of Birmingham, 20 Jun. 2016. <em><a href="https://www.youtube.com/watch?v=Oo4MwLxnMv4">Youtube</a>, </em>time index 20:40-21:07. [<a href="#identifier_31_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_32_2155" class="footnote">An authoritative and readable overview of this system and its history is John Vice and Stephen Farrell, <a href="http://www.parliament.uk/documents/lords-library/History-of-Hansard.pdf"><em>The History of Hansard</em></a> (London: House of Lords Hansard and the House of Lords Library, 2017). [<a href="#identifier_32_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_33_2155" class="footnote">See "<a href="http://www.hansard-archive.parliament.uk/">Hansard Archive (debates from 1803)</a>." [<a href="#identifier_33_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_34_2155" class="footnote">Some other manipulations of this data were necessary and should be noted. The text files are kept at <a href="http://www.hansard-archive.parliament.uk/">http://www.hansard-archive.parliament.uk/</a>, but the index to sittings per volume is at <a href="https://api.parliament.uk/historic-hansard/volumes/index.html">https://api.parliament.uk/historic-hansard/volumes/index.html</a>. The two resources are not perfectly aligned; 40 volumes of Hansard are listed in the index but not present in the zipped files, while 24 volumes had a zipped text file but did not appear in the index. In the data set these volumes are simply omitted. Another five volumes had both index entries and a file but the dates of the sittings were listed as "unknown." For these I inserted dates between those of the volumes that came before and after, as follows: s1v9 (15-8-1807 to 20-1-1808), s1v13 (8-3-1809 to 10-4-1809), s1v21 (25-7-1811 to 16-3-1812), s1v24 (5-5-1812 to 3-11-1813), s3v108 (2-8-1849 to 25-2-1850). Finally, the Hansard files download heavily laden with xml codes; to make sure compression was being done mostly on the original report texts these codes were removed using the text editor Notepad++. [<a href="#identifier_34_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_35_2155" class="footnote">Interestingly, at least one veteran newspaper reporter of this era appears to have hypothesized a necessary tradeoff between time pressure and textual density. Charles Ross, who reported Parliamentary debates for <em>The Times </em>from 1820 until 1883 and was its Chief of Reporters from the mid-1850s, told a Parliamentary subcommittee in July 1878 that deadline pressures had made reporting "too much of a mechanical art at present; giving the mere words." Ross preferred that "there should be compression instead of expansion; you want a man's opinions, not his words," but told the subcommittee that "under the existing system it cannot well be done." Great Britain, Parliament, <em>Reports from Committees</em>, 1878. Vol. 8. "Report from the Select Committee on Parliamentary Reporting," 39. [<a href="#identifier_35_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_36_2155" class="footnote">Anthony Trollope, <em>An Autobiography </em>(Oxford and New York: Oxford University Press, 1980). [<a href="#identifier_36_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_37_2155" class="footnote">I assure Project Gutenberg's legal representatives that the truncated text files were created for research purposes only and have not been shared. [<a href="#identifier_37_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_38_2155" class="footnote">This part of the project would not have originally been possible without technical help and advice provided through a 2016 Advanced Collaborative Support Grant from the HathiTrust Research Center. Grateful thanks are due to Peter Organisiak, Ryan Dubnicek, Eleanor Dickson Koehl, and Nandana Nallapu. [<a href="#identifier_38_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_39_2155" class="footnote">This "little words" analysis was pioneered by J. F. Burrows, <em>Computation into Criticism: A Study in Jane Austen's Novels and an Experiment in Method </em>(Oxford: Oxford University Press, 1987). For modern discussion and application of stylistic analysis using modern statistical software see Matthew Jockers, <em>Macroanalysis: Digital Methods and Literary History </em>(Chicago: University of Illinois Press, 2013). [<a href="#identifier_39_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_40_2155" class="footnote">eBooks@Adelaide, University of Adelaide Library, <a href="https://ebooks.adelaide.edu.au">https://ebooks.adelaide.edu.au</a> [<a href="#identifier_40_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_41_2155" class="footnote">The fascinating story of how and why philosophers and humanists in the 1940s and 1950s briefly engaged and then disengaged Shannon's Information Theory is well told in Bernard Dionysius Geoghegan, "From Information Theory to French Theory: Jakobson, Levi-Strauss, and the Cybernetic Apparatus," <em>Critical Inquiry </em>38.1 (2011): 96-126. [<a href="#identifier_41_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_42_2155" class="footnote">See, for one particularly convinced anti-Information Theory example, Carl H. Weaver and Garry L. Weaver, "Information Theory and the Measurement of Meaning," <em>Speech Monographs </em>32 (1965): 435-447. [<a href="#identifier_42_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_43_2155" class="footnote">John Gunders, "<a href="http://www.doubledialogues.com/article/signal-or-noise-information-theory-and-the-novel/">Signal or Noise? Information Theory and the Novel</a>," <em>Double Dialogues </em>3 (Summer 2002). [<a href="#identifier_43_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_44_2155" class="footnote">For good explications of this position see William R. Paulson, <em>The Noise of Culture: Literary Texts in a World of Information </em>(Ithaca and London: Cornell University Press, 1988), and Philipp Schweighauser's entry on "Information Theory" in Bruce Clark and Manuela Rossini, eds, <em>The Routledge Companion to Literature and Science </em>(New York: Routledge, 2012), 145-156. [<a href="#identifier_44_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_45_2155" class="footnote">Paulson, 65. [<a href="#identifier_45_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_46_2155" class="footnote">David Letzler, "Crossed-Up Disciplinarity: What Norbert Wiener, Thomas Pynchon, and William Gaddis Got Wrong About Entropy and Literature," <em>Contemporary Literature </em>56.1 (2015), 52. [<a href="#identifier_46_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_47_2155" class="footnote">Norvig's readable essay evaluating Chomsky's continuing claims (as recently as 2011) that human language cannot usefully be statistically modeled is, "On Chomsky and the Two Cultures of Statistical Learning," reproduced at Norvig's website, <a href="http://norvig.com/chomsky.html">http://norvig.com/chomsky.html</a> [<a href="#identifier_47_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_48_2155" class="footnote">Steven T. Piantadosi, Harry Tily, and Edward Gibson, "<a href="http://www.pnas.org/cgi/doi/10.1073/pnas.1012551108">Word lengths are optimized for efficient communication</a>," <em>PNAS </em>108.9 (2011); and Steven T. Piantadosi, Harry Tily, and Edward Gibson, "The Communicative Function of Ambiguity in Language," <em>Cognition </em>122 (2012): 280-291. [<a href="#identifier_48_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_49_2155" class="footnote">Kyle Mahowald, Evelina Fedorenko, Steven T. Piantadosi, and Edward Gibson, "Info/information theory: Speakers choose shorter words in predictive contexts," <em>Cognition </em>126 (2013): 313-318. [<a href="#identifier_49_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_50_2155" class="footnote">Yang Xu and David Reiter, "Information Density Converges in Dialogue: Towards an Information-Theoretic Model," <em>Cognition </em>170 (2018), 147-163. [<a href="#identifier_50_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_51_2155" class="footnote">Rolf A. Zwaan, "Effect of Genre Expectations on Text Comprehension," <em>Journal of Experimental Psychology: Learning, Memory, and Cognition </em>20 (1994): 920-933. [<a href="#identifier_51_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_52_2155" class="footnote">Franco Moretti, "Style, Inc. Reflections on Seven Thousand Titles (British Novels, 1740-1850)," <em>Critical Inquiry </em>36 (2009): 134-158. [<a href="#identifier_52_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_53_2155" class="footnote">Andrew Piper, "<a href="http://culturalanalytics.org/2016/12/fictionality/">Fictionality</a>," <em>CA: Journal of Cultural Analytics </em>20 Dec. 2016. Article DOI: <a href="https://doi.org/10.22148/16.011">10.22148/16.011</a>, Dataverse DOI:  <a href="http://dx.doi.org/10.7910/DVN/5WKTZV">10.7910/DVN/5WKTZV</a> [<a href="#identifier_53_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_54_2155" class="footnote">See for example D. Biber, S. Conrad, and V. Cortes, "<em>If you look at ... : </em>Lexical bundles in university teaching and textbooks," <em>Applied Linguistics </em>25 (2004): 371-405. [<a href="#identifier_54_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_55_2155" class="footnote">Vladimir Nabokov, <em>Nicolai Gogol </em>(New York: New Directions, 1961), 56. [<a href="#identifier_55_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_56_2155" class="footnote">Raymond Chandler, <em>Raymond Chandler Speaking, </em>ed. Dorothy Gardiner and Kathrine Sorley Walker (Berkeley and Los Angeles: University of California Press, 1997), 119. [<a href="#identifier_56_2155" class="footnote-link footnote-back-link">↩</a>]</li><li id="footnote_57_2155" class="footnote">M.M. Bakhtin, <em>The Dialogic Imagination: Four Essays </em>(Austin: University of Texas Press, 1981); Lisa Zunshine, <em>Why We Read Fiction: Theory of Mind and the Novel </em>(Columbus: Ohio State University Press, 2006). [<a href="#identifier_57_2155" class="footnote-link footnote-back-link">↩</a>]</li></ol></div>